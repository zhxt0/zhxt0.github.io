---
layout: post
title: PySpark 常用语句
category: 工具 
keywords: PySpark 
---
以下语句可在pyspark-shell中尝试执行



载入parquet数据：

```
from pyspark.sql import HiveContext
sqlContext = HiveContext(sc)
path = "/path/to/data"
df = sqlContext.read.parquet(path)
```

载入保存的rdd数据，load100条

```
data = sc.textFile("/path/to/data")
data.count()
data = data.take(100) # list
data100 = sc.parallelize(data)# list to rdd
data100.count()
data100.saveAsTextFile("/path/to/save")
```

创建dataframe,注册临时表,并执行spark sql

```
df = spark.range(15).toDF("num")
df.registerTempTable("table")
df2 = spark.sql("select num, sqrtValue(cast(num as float)) as sqrt_num from table")
```

保存dataframe

```
TODO
```

list创建dataframe

```
TODO
```

